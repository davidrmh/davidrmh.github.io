---
title: "Producto escalar y valores característicos"
author: "David R. Montalván Hernández"
subtitle: "Módulo 2"
output: 
  revealjs::revealjs_presentation:
    theme: sky
    highlight: kate
    transition: fade
    incremental: false
    center: true
    self_contained: false
    reveal_plugins: ["chalkboard"]
    reveal_options:
      slideNumber: true
      touch: true
---

En estas notas se repasarán algunos resultados relacionados con el producto escalar y los vectores característicos (eigen vectores).

# **Producto escalar**

## **Producto escalar**

Sea $\mathbb{V}$ un espacio vectorial sobre un campo $\mathbb{F}$. Un **producto escalar** (o producto interno) es un mapeo $\left<\cdot,\cdot \right>:\mathbb{V} \times \mathbb{V} \rightarrow \mathbb{F}$ que satisface las siguiente propiedades:

* $\left< \mathbf{v}, \mathbf{v} \right> \geq 0$, con igualdad si y sólo si $\mathbf{v} = \mathbf{0}$

* $\left< \mathbf{v}, \mathbf{w} \right> = \left< \mathbf{w}, \mathbf{v} \right>$ para todo $\mathbf{v}, \mathbf{w} \in \mathbb{V}$

* $\left< \mathbf{u}, \mathbf{v} + \mathbf{w} \right> = \left< \mathbf{u}, \mathbf{v} \right> + \left< \mathbf{u}, \mathbf{w} \right>$

* Si $k \in \mathbb{F}$, entonces:

$$ \left< k \mathbf{u}, \mathbf{v} \right> = k \left< \mathbf{u}, \mathbf{v} \right> = \left< \mathbf{u}, k\mathbf{v} \right> $$

## **Norma de un vector**

Sea $\mathbb{V}$ un espacio vectorial sobre un campo $\mathbb{F}$. Sea $\left<\cdot,\cdot \right>$ un producto escalar definido en $\mathbb{V}$. La **norma** de un vector $\mathbf{v} \in \mathbb{V}$ se define como:


$$ ||\mathbf{v}|| = \sqrt{\left< v, v \right>}.$$

## **Propiedades de una norma**
* $||\mathbf{v}|| = 0$ si y sólo si $\mathbf{v} = \mathbf{0}$.
* $||k \mathbf{v}|| = |k| ||\mathbf{v}||$, para $k \in \mathbb{F}, \mathbf{v} \in \mathbb{V}$ 

* $\left< \mathbf{v}, \mathbf{w} \right>^{2} \leq ||\mathbf{v}||^{2}  ||\mathbf{w}||^{2}$ con igualdad si y sólo si $\mathbf{v} = k\mathbf{w}$ para algún escalar $k \in \mathbb{F}$ (Desigualdad de Cauchy-Schwarz)

* $||\mathbf{v} + \mathbf{w} || \leq ||\mathbf{v}|| + ||\mathbf{w}||$ (Desigualdad del triángulo)

* $||\mathbf{v} + \mathbf{w}||^2 + ||\mathbf{v} - \mathbf{w}||^2 = 2(||\mathbf{v}||^2 + ||\mathbf{w}||^2)$ (Ley del paralelogramo)

## **Vectores unitarios**

Un vector $\mathbf{v}$ en un espacio vectorial $\mathbb{V}$ sobre el cual se ha definido una norma, se dice que es un **vector unitario** (o vector dirección), si $||\mathbf{v}|| = 1$.

## **Ejemplos de normas**

* Norma $L^p$ para $p \geq 1$: $|| \mathbf{x} ||_p = \left(\sum_{i} |x_i|^p \right)^{\frac{1}{p}}$

* "Norma" $L^0$:  El número de elementos en $\mathbf{x}$ que son distintos de cero

* Norma $L^\infty$: $|| \mathbf{x} ||_\infty = \max_{i} |x_i|$

* Norma de Frobenius: Si $\mathbf{A}$ es una matriz, entonces $||\mathbf{A}||_{F} =\sqrt{\sum_{i,j} A_{i,j}^2}$, en particular, si $\mathbf{A}$ es una matriz columna o renglón, la norma de Frobenius es la norma $L^2$.

## **Ejercicios**

* Implemente cada una de las normas anteriores de la siguiente forma:
    * Sin utilizar numpy
    * Utilizando numpy (sin utilizar el módulo ```linalg```)
    
pruebe su código con el vector $\mathbf{x} = (-1, 0, 4, 0, 26, 18, -120)$ y con la matriz
$$
\mathbf{A}=
\begin{pmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{pmatrix}
$$

* Demuestre que lo siguiente es un producto escalar
$$
\left<f , g\right> = \int_{0}^{1} f(t)g(t)dt
$$



## **Ejercicios**

* Demuestre la desigualdad del triángulo. **Sugerencia:** exprese $||\mathbf{v} + \mathbf{w}||^2$ en términos del producto escalar

* Demuestre que para todo $\mathbf{v} \neq \mathbf{0}$, el vector

$$
\mathbf{w} = \dfrac{\mathbf{v}}{||\mathbf{v}||}
$$

es un vector unitario

* Demuestre que la "norma" $L^0$ no es propiamente una norma


## **Normas con numpy**

con numpy, es posible calcular una variedad de normas tanto para matrices como para vectores, utilizando la función ```norm``` del módulo ```linalg```

```python
import numpy as np
x = np.array([-1, 0, 4, 0, 26, 18, -120])
A = np.array([ [1,2,3], [4, 5, 6] ])
help(np.linalg.norm)

#norma l1 (para un vector)
print(np.linalg.norm(x, ord = 1))

#norma l2 (para un vector)
print(np.linalg.norm(x, ord = 2))

#norma L infinito (vector)
print(np.linalg.norm(x, ord = np.inf))

#norma de Frobenius para un vector
print(x.shape)
#np.linalg.norm(x, ord = 'fro') ERROR!!!
x = x.reshape(x.shape[0], 1)
print(x.shape)
print(np.linalg.norm(x, ord = 'fro'))
print(np.linalg.norm(x, ord = 'fro') == np.linalg.norm(x, ord = 2) )

#norma de Frobenius para una matriz
print(np.linalg.norm(A, ord = 'fro'))
```

## Importancia de la norma $L^2$

* Conjunto de $n$ observaciones de su experimento $\mathbf{y}_{target} \in \mathbb{R}^n$.

* Conjunto de predicciones hechas por su modelo $\mathbf{y}_{model} \in \mathbb{R}^n$.

* Idealmente queremos $\mathbf{y}_{model} \approx \mathbf{y}_{target}$.

$$
Loss(\mathbf{y}_{target},  \mathbf{y}_{model}(\omega)) = \dfrac{1}{2n} ||\mathbf{y}_{target} - \mathbf{y}_{model}||_{2}^{2} 
$$

Esta cantidad es llamada el **error cuadrático medio**.

# **Ortogonalidad**


## **Ortogonalidad**

Dos vectores son **ortogonales** si

$$\left<\mathbf{v}, \mathbf{w} \right> = 0.$$

Dos vectores son **paralelos** si

$$\left<\mathbf{v}, \mathbf{w} \right> = ||\mathbf{v}|| \times ||\mathbf{w}||$$ (El coseno del ángulo que forman es $1$)

## **Conjuntos ortonormales**

Sea $\mathcal{X} = \{\mathbf{v_1}, \ldots, \mathbf{v_n} \}$ un conjunto de vectores distintos del vector cero. $\mathcal{X}$ es un **conjunto ortogonal** si $\left< \mathbf{v_i}, \mathbf{v_j} \right> = 0$ para todo $i \neq j$.

Además, si $||\mathbf{v_i}|| = 1$ para toda $i$, $\mathcal{X}$ es un **conjunto ortonormal**.

**Nota:** En la literatura no siempre se distingue entre ortogonal y ortonormal, suele decirse únicamete ortogonal (entendiéndose que se habla de vectores unitarios).

## **Bases ortogonales**

* **Base ortogonal**  una base formada por un conjunto de vectores ortogonales.

* **Base ortonormal**  una base formada por un conjunto de vectores ortonormales.

## **Matrices ortogonales y ortonormales**

Sea $\mathbf{A}$ una matriz de $n \times n$

* $\mathbf{A}$ es ortogonal si sus columnas forman un conjunto ortogonal.

* $\mathbf{A}$ es ortonormal si sus columnas forman un conjunto ortonormal.

## **Matriz inversa y ortonormalidad**

Sea $\mathbf{A}$ una matriz de $n \times n$, lo siguiente es equivalente:

1. Las columnas de $\mathbf{A}$ son vectores ortonormales.

2. $\mathbf{A}^{T} \mathbf{A} = \mathbf{I} = \mathbf{A} \mathbf{A}^{T}.$

3. Los renglones de $\mathbf{A}$ son vectores ortonormales.

Por lo tanto, para una matriz ortonormal, $\mathbf{A}^{-1} = \mathbf{A}^T$.

## **Ejercicios**

* Del resultado anterior, demuestre que 1 implica 2 y que 2 implica 3. Demostrar que 3 implica 1 es más complicado y por lo tanto se omite.

* Sea $\{\mathbf{v_1}, \ldots, \mathbf{v_n} \}$ un conjunto de vectores ortogonales, demuestre que
$$
\left| \left| \sum_{i = 1}^{n} \mathbf{v_i} \right| \right|^{2} = \sum_{i = 1}^{n}||\mathbf{v_i}||^{2}.
$$
Sugerencia: Primero analice el caso en que $n = 2$ y argumente la generalización.

## **Ejercicios**

* Demuestre que un conjunto ortogonal de vectores distintos del vector cero, $\mathcal{X} = \{\mathbf{v_1}, \ldots, \mathbf{v_n} \}$, forma un conjunto linealmente independiente. El recíproco de esta proposición ¿es verdadero?

# **Valores y vectores característicos**

## **Valores y vectores característicos**

Sea $\mathbf{A}$ una matriz de $n \times n$. Un **valor característico** (eigenvalue) de $\mathbf{A}$ es un escalar tal que satisface la ecuación

$$
\mathbf{Ax} = \lambda \mathbf{x}
$$

para un vector $\mathbf{x} \neq \mathbf{0}$ (**vector característico** o eigenvector).

¿Cuál sería la interpretación geométrica?

## 

$\lambda$ es un valor característico de $\mathbf{A}$ si y sólo si el sistema (homogéneo) de ecuaciones lineales $\left( \mathbf{A} - \lambda \mathbf{I} \right)$ tiene una solución no trivial $\mathbf{x} \neq \mathbf{0}$. Cualquier $\lambda$ que haga que la matriz  $\mathbf{A} - \lambda \mathbf{I}$ sea singular, será un valor característico de la matriz $\mathbf{A}$. De manera equivalente, basta  encontrar un $\lambda$, tal que $|\lambda \mathbf{I} - \mathbf{A}| = 0.$

## **Polinomio característico**

El polinomio característico de una matriz $\mathbf{A}$ de $n \times n$, es el polinomio dado por

$$
p_{\mathbf{A}}(\lambda) = \left| \lambda \mathbf{I} - \mathbf{A} \right|
$$

Este polinomio es un polinomio de grado $n$ el cual puede tener $n$ raíces (teorema fundamental del álgebra) algunas pueden ser raíces complejas y otras reales.

## **Ejercicios**

* En la definición de valores característicos, ¿es necesario que la matriz $\mathbf{A}$ sea una matriz cuadrada?

* Encuentre los valores y vectores característicos de la matriz

$$
\begin{pmatrix}
0 & 1 \\
-2 & -3
\end{pmatrix}
$$

```python
import numpy as np
help(np.roots)
``` 
## **Ejercicios**

* Encuentre los valores característicos de una matriz triangular superior de $n \times n$.

##

con numpy es posible obtener los vectores y valores caraterísticos de una matriz utilizando la función ```eig``` del módulo ```linalg```

```python
import numpy as np
A = np.array([ [0, 1], [-2, -3] ])
help(np.linalg.eig)

eig_val, eig_vec = np.linalg.eig(A)

#validamos
print(np.allclose(A @ eig_vec[:, 0], eig_val[0] * eig_vec[:, 0]))
print(np.allclose(A @ eig_vec[:, 1], eig_val[1] * eig_vec[:, 1]))
```
## **Propiedades**

Para una matriz cuadrada $\mathbf{A}$ tenemos

* Si $\lambda$ es un valor característico de la matriz $\mathbf{A}$, entonces también es un valor característico de $\mathbf{A}^{T}$

* El producto de los valores característicos de $\mathbf{A}$, es igual a $|\mathbf{A}|$

* La suma de los valores característicos de $\mathbf{A}$, es igual a la **traza** de $\mathbf{A}$, $tr(\mathbf{A})$, es decir, igual a la suma de los elementos en la diagonal de $\mathbf{A}$

$$
tr(\mathbf{A}) = \sum_{i = 1}^{n} a_{ii}
$$

##

* Si $\{\lambda_{1}, \lambda_{2}, \ldots, \lambda_{k} \}$ es un conjunto de valores característicos de $\mathbf{A}$, todos ellos distintos, entonces el conjunto con los vectores característicos $\{\mathbf{x}_1, \ldots, \mathbf{x}_k \}$ es un conjunto linealmente independiente.

* Si $\mathbf{A}$ es una matriz **simétrica** sobre el campo $\mathbb{R}$, entonces todos sus valores característicos pertenecen a $\mathbb{R}$

* Si $\mathbf{x}_1$ y $\mathbf{x}_2$ son dos vectores característicos, distintos, de una matriz real simétrica $\mathbf{A}$ y $\lambda_1, \lambda_2$ son sus respectivos valores característicos. Entonces $\mathbf{x}_1$ y $\mathbf{x}_2$ son ortogonales.

## **Descomposición espectral**

Si $\mathbf{A}$ es una matriz simétrica sobre el campo $\mathbb{R}$, entonces existe una matriz ortonormal $\mathbf{P}$ y una matriz diagonal $\mathbf{\Lambda}$ tales que

$$
\mathbf{P}^{T} \mathbf{A} \mathbf{P} = \mathbf{\Lambda}
$$

La diagonal de $\mathbf{\Lambda}$ está formada por los valores característicos de $\mathbf{A}$ y las columnas de $\mathbf{P}$ son los vectores característicos correspondientes.